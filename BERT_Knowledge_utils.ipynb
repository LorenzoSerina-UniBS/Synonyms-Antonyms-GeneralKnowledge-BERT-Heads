{"cells":[{"cell_type":"markdown","metadata":{"id":"8OYOdf3JJqf_"},"source":["# Function Definition and packages download"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26874,"status":"ok","timestamp":1685353974781,"user":{"displayName":"LORENZO SERINA","userId":"00482800622160368163"},"user_tz":-120},"id":"8_DxzQ1gt-rm","outputId":"f16c175c-8803-4ca3-9c28-ff28ea040a2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["#! pip install pandas==1.5.3"],"metadata":{"id":"FDsJRG-cxrsA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrHIvmVDXxap"},"outputs":[],"source":["import json\n","\n","def read_json_keys(filepath):\n","    with open(filepath) as f:\n","        data = json.load(f)\n","    return list(data.keys())\n","\n","def read_json(filepath):\n","  with open(filepath) as f:\n","        return json.load(f)\n","\n","\n","def save_dict_to_json_file(data, filepath):\n","    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(data, f, indent=2, ensure_ascii=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhvkiluAac3k"},"outputs":[],"source":["def is_number(s):\n","    try:\n","        float(s)\n","        return True\n","    except ValueError:\n","        return False\n","\n","import re\n","\n","def extract_decimal(s):\n","    # Use a regular expression to search for a decimal number\n","    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n","    if match:\n","        return float(match.group(0))\n","    else:\n","        return None\n","\n","        \n","import statistics\n","from typing import List\n","\n","def calculate_statistics(numbers: List[float]) -> dict:\n","    statistics_data = {}\n","    statistics_data['mean'] = statistics.mean(numbers)\n","    statistics_data['median'] = statistics.median(numbers)\n","    try:\n","        statistics_data['mode'] = statistics.mode(numbers)\n","    except statistics.StatisticsError:\n","        statistics_data['mode'] = None\n","    statistics_data['stdev'] = statistics.stdev(numbers)\n","    statistics_data['variance'] = statistics.variance(numbers)\n","    return statistics_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vaapWEjZKHRv"},"outputs":[],"source":["import os\n","\n","def read_documents_in_folder(folder_path: str) -> list:\n","    document_paths = []\n","    for root, dirs, files in os.walk(folder_path):\n","        for file in files:\n","              document_path = os.path.join(root, file)\n","              document_paths.append(document_path)\n","    return document_paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ovnDvBN0KRj_"},"outputs":[],"source":["import pandas as pd\n","import pickle\n","\n","def create_dataset_pickle(lista, path):\n","    dataset = pd.DataFrame(lista, columns=['text'])\n","    with open(path, 'wb') as f:\n","        pickle.dump(dataset, f)"]},{"cell_type":"markdown","metadata":{"id":"V1QZc01VJxFs"},"source":["\n","# Filter the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJ2d56pZZkbP"},"outputs":[],"source":["from scipy.stats import pearsonr\n","\n","path_risultati_dir=\"\"\n","path_dir=\"\"\n","\n","path_completo=path_dir\n","path_risultati= path_risultati_dir\n","with open(path_completo) as f:\n","  data = json.load(f)\n","new_risultati={}\n","for k in read_json_keys(path_completo):\n","  print(k)\n","  smear=[]\n","  wn=[]\n","  new_risultati[k]={}\n","  new_risultati[k][\"Risultati totali\"]=[]\n","  count=0\n","  #print(data[k])\n","  for i in data[k]['risultati_totali']:\n","    if is_number(i[0]) or is_number(i[2]):\n","      pass\n","    else:\n","      if i not in new_risultati[k][\"Risultati totali\"] and  i[0] != i[2] and len(i[0])>1 and len(i[2])>1:\n","        new_risultati[k][\"Risultati totali\"].append(i)\n","        count+=1\n","        wn.append(extract_decimal(i[4]))\n","        smear.append(extract_decimal(i[5]))\n","  new_risultati[k][\"Numero coppie\"]= count\n","  if len(wn)>1:\n","    new_risultati[k][\"Wordnet\"]= calculate_statistics(wn)\n","    new_risultati[k][\"Smear\"]= calculate_statistics(smear)\n","    correlation, _ = pearsonr(wn, smear)\n","    new_risultati[k][\"Correlation\"]=correlation\n","\n","save_dict_to_json_file(new_risultati, path_risultati)"]},{"cell_type":"markdown","source":["# Analyse the results"],"metadata":{"id":"4f-GQ69q97ru"}},{"cell_type":"code","source":["from collections import OrderedDict\n","from random import sample\n","import matplotlib.pyplot as plt\n","\n","def plot_line_from_dict(data_dict, path):\n","    \"\"\"\n","    Plots a line graph from a dictionary where each key is on the x-axis and its value is on the y-axis.\n","    \"\"\"\n","    # Get the keys and values from the dictionary\n","    x_values = list(data_dict.keys())\n","    y_values = list(data_dict.values())\n","\n","    # Set up the figure and axis\n","    #fig, ax = plt.subplots()\n","    fig, ax = plt.subplots(figsize=(15, 5))\n","\n","    # Plot the line\n","    ax.plot(x_values, y_values)\n","    plt.xticks(rotation=90)\n","\n","\n","    # Show the plot\n","    fig.savefig(path)\n","    plt.show()\n","\n","\n","path=\"\"\n","path_model=path+\"\" #folder where the data of that model are stored\n","data=read_json(path_model+\"\") #Where the results are stored\n","chiavi=read_json(path+\"\")#Where the datasets are stored\n","path_risultati=path_model+\"risultati_anto.json\"\n","diag_path=\"diag_anto.json\"\n","tot=len(chiavi)\n","print(tot)\n","risultati={}\n","\n","risultati={}\n","risultati[\"Total\"]=0\n","risultati[\"Top 5\"]=0\n","risultati[\"Top Self\"]=0\n","\n","\n","visti_tot=[]\n","for h in data:\n","  visti=[]\n","  risultati[h]={}\n","  #visti=[]\n","  #print(h)\n","  risultati[h][\"Numero coppie heads\"]=len(data[h][\"risultati_totali\"])\n","  risultati[h][\"Numero\"]=0\n","  risultati[h][\"Corrette\"]=[]\n","  for t in data[h]:\n","    if \"train\" in t:\n","      for chiave in chiavi:\n","        #print(chiave)\n","        frase_chiave=chiavi[chiave].lower().strip()\n","        if frase_chiave[-1]!=\".\":\n","          frase_chiave+=\".\"     \n","        if frase_chiave == data[h][t][\"sentence\"].lower().strip():\n","          #print(p, \": \",chiavi[chiave][p].lower().strip(), \"      \",data[h][t][\"sentence\"].lower().strip() )\n","          for r in data[h][t][\"risultati\"]:\n","            c=chiave.split(\", \")#\", \"\n","            if (r[0].lower() == c[0].lower() and r[2].lower() in c[1].lower()) or (r[0].lower() in c[1].lower() and r[2].lower()==c[0].lower()): #Vedi linea sopra per errore\n","              #print(h,\": \",r[0], r[2], \"---\", c)\n","              if ((c[0].lower(),c[1].lower()) not in visti):\n","                risultati[h][\"Numero\"]+=1\n","                risultati[h][\"Corrette\"].append(c)\n","                visti.append((c[0].lower(),c[1].lower()))\n","                if ((c[0].lower(),c[1].lower()) not in visti_tot):\n","                  visti_tot.append((c[0].lower(),c[1].lower()))\n","                  risultati[\"Totali\"]+=1\n","\n","\n","  if risultati[h][\"Numero coppie heads\"]==0:\n","      risultati[h][\"Precision\"]=0\n","  else:\n","    risultati[h][\"Precision\"]=risultati[p][h][\"Numero\"]/risultati[h][\"Numero coppie heads\"]\n","  risultati[h][\"Recall\"]=risultati[h][\"Numero\"]/tot\n","  if risultati[h][\"Precision\"] ==0 or risultati[h][\"Recall\"]==0:\n","    risultati[h][\"F1\"]=0\n","  else:\n","    risultati[h][\"F1\"]=2*risultati[h][\"Precision\"]*risultati[h][\"Recall\"]/(risultati[h][\"Precision\"]+risultati[h][\"Recall\"])\n","\n","  lista=[]\n","  diag=read_json(path+diag_path)\n","  keys=list(diag.keys())\n","  #rand=sample(keys,20)\n","  for k in keys[:20]:\n","    #print(k)\n","    for coppia in risultati[k][\"Corrette\"]:\n","      if coppia not in lista:\n","        lista.append(coppia)\n","  risultati[\"Top Diagonal\"]=len(lista)\n","\n","\n","  show_dict={}\n","  for d in diag:\n","    show_dict[d]=risultati[p][d][\"F1\"]\n","  ord=dict(OrderedDict(sorted(show_dict.items(), key=lambda t: t[1], reverse=True)))\n","  keys=list(ord.keys())[:20]\n","  #print(keys)\n","  lista=[]\n","  for k in keys:\n","    print(k, ord[k])\n","    for coppia in risultati[p][k][\"Corrette\"]:\n","      if coppia not in lista:\n","        lista.append(coppia)\n","  risultati[p][\"Top 5\"]=len(lista)\n","  print(\"\\n\\n\\n\")\n","\n","save_dict_to_json_file(risultati, path_risultati)"],"metadata":{"id":"MNBQUrtm9_Ad"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["8OYOdf3JJqf_","V1QZc01VJxFs","DciPACrfKpKd","lFNkyOCbNai_","vWclEMvacdCQ","mfScPGDENhUJ","0ZmgXs6XUZW5","nvOsLOmHP-sv","mQzlZZPi5yl8","S71bJaFlyLB3"],"gpuType":"T4","mount_file_id":"1vYDjI9hg-Da7sTMjlr4xP6nhiL0cMd3Q","authorship_tag":"ABX9TyO386pQSzK+F2WilYg+8cfS"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}